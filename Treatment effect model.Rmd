---
title: "Modeling"
date: "November 11, 2016"
output: word_document
---
##Predict whether a salesperson self selects to take service training


For this project, my group and I wanted to analyze the impact of salesperson training on sales. Does sales training increase sales and decrease returns?
Is sales training worth it? There are 2 types of training modules in our dataset: Product knowledge and service selling training. Is taking one of the training modules more effective in increasing sales than taking the other? Product knowledge vs service selling?

#Clean existing data, set dummy variables
```{r}
setwd("/Users/Karen/Desktop/Grad School/Econometrics with R/Final Project")
mydata <- read.csv("FullAggregateData.csv")
length(unique(mydata$SA_ID))

yr2013$Annual.SaleAmt[which(is.nan(yr2013$Annual.SaleAmt))] = NA


yr2013<-subset(mydata, FY==2013)
yr2013$ServiceModuleCompleted[is.na(yr2013$ServiceModuleCompleted)] <- as.numeric(0)
yr2013$StoreSqFt<-NULL
yr2013$Competitor<-NULL
yr2013$MallGrade<-NULL
yr2013<-yr2013[!(yr2013$Annual.SaleAmt==0),]
yr2013<-yr2013[!is.na(yr2013$SA_gender), ] #Gender has a few NAs (212)

```


##Testing Effect of Service Module Training on Sale Amount ($)
```{R}


m9<- lm(log(Annual.SaleAmt)~num_PKModulesCompleted*ServiceModuleCompleted + factor(MallGrade)+factor(SA_gender) + SA.service_yrs + SA.Pay+ Cust.GenderRatio+PT, data=yr2013)
summary(m9)


##Suspect that there is a selection problem with the Service Module Completed variable. Need to run selection model.
```

##Selection Model- Probit
```{r}
probit1<- glm(ServiceModuleCompleted~ PT +factor(SA_gender) + SA.service_yrs+ log(SA.Pay), data=yr2013, family=binomial(link="probit")) 
probit2<- glm(ServiceModuleCompleted~ PT +factor(SA_gender) +  log(SA.Pay), data=yr2013, family=binomial(link="probit")) 

library(lmtest)
lrtest(probit1,probit2)


with(probit1, null.deviance - deviance)
with(probit1, df.null - df.residual)
with(probit1, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE))



##Predict based on probit, compare to actual. If over 0.7 then good
library(lmtest)
pred = predict(probit2, data=yr2013) # Generate predicted probabilities
return_prediction <- ifelse(pred >= 0.5,1,0) # If the predicted probability is greater than 0.5, then the predicted classification will be a return (return==1), otherwise it will be a no return (return==0)
misClasificError <- mean(return_prediction != yr2013$ServiceModuleCompleted) 
# count number of wrong classifications
print(paste('Accuracy',1-misClasificError))

##Accuracy very good, 0.95

##Test marginal effect of selection model
library(mfx)

probitmfx(formula=ServiceModuleCompleted~ PT +factor(SA_gender) + log(SA.Pay), data=yr2013)


m11<-lm(log(Annual.SaleAmt)~num_PKModulesCompleted*ServiceModuleCompleted,data=yr2013)

library(effects)
plot(effect(term="num_PKModulesCompleted:ServiceModuleCompleted",mod=m11,default.levels=2),main = "Effect of Training Modules on Sale Amount ($)", ylab="Annual Sales Amount", xlab="Number of PK Modules Completed",  multiline=TRUE)


```


## Treatment-effect model 
```{r}

library(AER)
library(foreign)
library(sampleSelection)

CB <- function(x) {
   ifelse(x > -500,
          -exp(dnorm(x, log = TRUE)
                - pnorm(x, log.p = TRUE))*x
           -exp(2*(dnorm(x, log = TRUE) - pnorm(x, log.p = TRUE))),
          -1)
}

lambda <- function(x) {
   as.vector(ifelse(x > -30, dnorm(x)/pnorm(x), -x))
                           # can we prove it?
}

tobitTfit <- function(YS, XS, YO, XO, start,
                      weights=NULL, print.level=0,
                      maxMethod="Newton-Raphson",
                      index=NULL,
                      binaryOutcome=FALSE,
                      ...) {
### Tobit treatment models:
### The latent variable is:
### YS* = XS'g + u
### The observables are:
###      / 1  if  YS* > 0
### YS = \ 0  if  YS* <= 0
### YO = X'b + YS bT + v
### u, v are correlated
### 
### Arguments:
### 
###  YS        binary or logical vector, 0 (FALSE) and 1 (TRUE)
###  XS              -"-                selection, should include
###              exclusion restriction
###  YO        numeric vector, outcomes
###  XO        explanatory variables for outcomes, should include YS
###  index     individual parameter indices in the parameter vector.
###            Should always be supplied but can generate here for
###            testing purposes
###  ...       additional parameters for maxLik
###
   loglik <- function( beta) {
      betaS <- beta[iBetaS]
      betaO <- beta[iBetaO]
      sigma <- beta[iSigma]
      if(sigma <= 0) return(NA)
      rho <- beta[iRho]
      if( ( rho < -1) || ( rho > 1)) return(NA)
                           # check the range
      XS0.betaS <- XS0%*%betaS
                           # denoted by 'z' in the vignette
      XS1.betaS <- XS1%*%betaS
      v0 <- YO0 - XO0%*%betaO
      v1 <- YO1 - XO1%*%betaO
      sqrt1r2 <- sqrt( 1 - rho^2)
      B0 <- (-XS0.betaS - rho/sigma*v0)/sqrt1r2
      B1 <- (XS1.betaS + rho/sigma*v1)/sqrt1r2
      loglik <- numeric(nObs)
      loglik[i0] <- -1/2*log( 2*pi) - log( sigma) -
          0.5*( v0/sigma)^2 + pnorm( B0, log.p=TRUE) 
      loglik[i1] <- -1/2*log( 2*pi) -log( sigma) -
          0.5*( v1/sigma)^2 + pnorm( B1, log.p=TRUE) 
      #sum(loglik)
      loglik
   }
   gradlik <- function(beta) {
      ## gradient is nObs x nParam matrix
      betaS <- beta[iBetaS]
      betaO <- beta[iBetaO]
      sigma <- beta[iSigma]
      if(sigma <= 0) return(NA)
      rho <- beta[iRho]
      if( ( rho < -1) || ( rho > 1)) return(NA)
                           # check the range
      XS0.betaS <- XS0%*%betaS
                           # denoted by 'z' in the vignette
      XS1.betaS <- XS1%*%betaS
      v0 <- drop(YO0 - XO0%*%betaO)
      v1 <- drop(YO1 - XO1%*%betaO)
      sqrt1r2 <- sqrt( 1 - rho^2)
      B0 <- (-XS0.betaS - rho/sigma*v0)/sqrt1r2
      B1 <- (XS1.betaS + rho/sigma*v1)/sqrt1r2
      lambda0 <- drop(lambda(B0))
      lambda1 <- drop(lambda(B1))
      ## now the gradient itself
      gradient <- matrix(0, nObs, nParam)
      gradient[i0, iBetaS] <- -lambda0*XS0/sqrt1r2
      gradient[i1, iBetaS] <- lambda1*XS1/sqrt1r2
      gradient[i0,iBetaO] <- (lambda0*rho/sigma/sqrt1r2
                              + v0/sigma^2)*XO0
      gradient[i1,iBetaO] <- (-lambda1*rho/sigma/sqrt1r2
                              + v1/sigma^2)*XO1
      gradient[i0,iSigma] <- (-1/sigma + v0^2/sigma^3
                              + lambda0*rho/sigma^2*v0/sqrt1r2)
      gradient[i1,iSigma] <- (-1/sigma + v1^2/sigma^3
                              - lambda1*rho/sigma^2*v1/sqrt1r2)
      gradient[i0,iRho] <- -lambda0*(v0/sigma + rho*XS0.betaS)/
          sqrt1r2^3
      gradient[i1,iRho] <- lambda1*(v1/sigma + rho*XS1.betaS)/
          sqrt1r2^3
#      colSums(gradient)
      gradient
   }
   hesslik <- function(beta) {
                           # This is a hack in order to avoid numeric problems
      ## gradient is nObs x nParam matrix
      betaS <- beta[iBetaS]
      betaO <- beta[iBetaO]
      sigma <- beta[iSigma]
      if(sigma <= 0) return(NA)
      rho <- beta[iRho]
      if( ( rho < -1) || ( rho > 1)) return(NA)
                           # check the range
      XS0.betaS <- XS0%*%betaS
                           # denoted by 'z' in the vignette
      XS1.betaS <- XS1%*%betaS
      v0 <- drop(YO0 - XO0%*%betaO)
      v1 <- drop(YO1 - XO1%*%betaO)
      sqrt1r2 <- sqrt( 1 - rho^2)
      B0 <- (-XS0.betaS - rho/sigma*v0)/sqrt1r2
      B1 <- (XS1.betaS + rho/sigma*v1)/sqrt1r2
      lambda0 <- drop(lambda(B0))
      lambda1 <- drop(lambda(B1))
      CB0 <- drop(CB(B0))
      CB1 <- drop(CB(B1))
      hess <- array(0, c( nParam, nParam))
      hess[,] <- NA
      hess[iBetaS,iBetaS] <-
         t( XS0) %*% ( XS0 * CB0)/sqrt1r2^2 +
             t( XS1) %*% ( XS1 * CB1)/sqrt1r2^2
      hess[iBetaS,iBetaO]  <-
         - t( XS0) %*% ( XO0 * CB0)*rho/sqrt1r2^2/sigma -
             t( XS1) %*% ( XO1 * CB1)*rho/sqrt1r2^2/sigma
      hess[iBetaO,iBetaS] <- t(hess[iBetaS,iBetaO])
      hess[iBetaS,iSigma] <-
         -rho/sigma^2/sqrt1r2^2*t( XS0) %*% ( CB0*v0) -
             rho/sigma^2/sqrt1r2^2*t( XS1) %*% ( CB1*v1)
      hess[iSigma,iBetaS] <- t(hess[iBetaS,iSigma])
      hess[iBetaS,iRho] <- 
         (t(XS0) %*% (CB0*(v0/sigma + rho*XS0.betaS)/sqrt1r2^4
                      - lambda0*rho/sqrt1r2^3) 
          +t(XS1) %*% (CB1*(v1/sigma + rho*XS1.betaS)/sqrt1r2^4
                       + lambda1*rho/sqrt1r2^3)
          )
      hess[iRho,iBetaS] <- t(hess[iBetaS,iRho])
      ##
      hess[iBetaO,iBetaO] <- 
         t( XO0) %*% (XO0*((rho/sqrt1r2)^2*CB0 - 1))/sigma^2 +
             t( XO1) %*% (XO1*( (rho/sqrt1r2)^2 * CB1 - 1))/sigma^2
      hess[iBetaO,iSigma] <-
         (t( XO0) %*% (CB0*rho^2/sigma^3*v0/sqrt1r2^2
                       - rho/sigma^2*lambda0/sqrt1r2 
                       - 2*v0/sigma^3) 
          + t( XO1) %*% (CB1*rho^2/sigma^3*v1/sqrt1r2^2 
                         + rho/sigma^2*lambda1/sqrt1r2
                         - 2*v1/sigma^3)
          )
      hess[iSigma,iBetaO] <- t(hess[iBetaO,iSigma])
      hess[iBetaO,iRho] <-
         (t(XO0) %*% (-CB0*(v0/sigma + rho*XS0.betaS)/sqrt1r2^4*rho
                      + lambda0/sqrt1r2^3)/sigma
          + t(XO1) %*% (-CB1*(v1/sigma + rho*XS1.betaS)/sqrt1r2^4*rho
                        - lambda1/sqrt1r2^3)/sigma
          )
      hess[iRho,iBetaO] <- t(hess[iBetaO,iRho])
      ##
      hess[iSigma,iSigma] <-
         (sum(1/sigma^2
             -3*v0*v0/sigma^4
             + v0*v0/sigma^4*rho^2/sqrt1r2^2*CB0
             -2*lambda0*v0/sqrt1r2*rho/sigma^3)
          + sum(1/sigma^2
                -3*v1*v1/sigma^4
                +rho^2/sigma^4*v1*v1/sqrt1r2^2*CB1
                +2*lambda1*v1/sqrt1r2*rho/sigma^3)
          )
      hess[iSigma,iRho] <- 
         (sum((-CB0*rho*(v0/sigma + rho*XS0.betaS)/sqrt1r2 + lambda0)
              *v0/sigma^2)/sqrt1r2^3
          - sum(
              (CB1*rho*(v1/sigma + rho*XS1.betaS)/sqrt1r2 + lambda1)
              *v1/sigma^2)/sqrt1r2^3
          )
      hess[iRho,iSigma] <- t(hess[iSigma,iRho])
      hess[iRho,iRho] <-
         (sum(CB0*( (v0/sigma + rho*XS0.betaS)/sqrt1r2^3)^2
              -lambda0*(XS0.betaS*(1 + 2*rho^2) + 3*rho*v0/sigma)/
                  sqrt1r2^5
              )
          + sum(CB1*( (v1/sigma + rho*XS1.betaS)/sqrt1r2^3)^2
                +lambda1*( XS1.betaS*( 1 + 2*rho^2) + 3*rho*v1/sigma) /
              sqrt1r2^5
                )
          )
      ## l.s2x3 is zero
      hess
   }
   ## ---------------
   NXS <- ncol( XS)
   if(is.null(colnames(XS)))
      colnames(XS) <- rep("XS", NXS)
   NXO <- ncol( XO)
   if(is.null(colnames(XO)))
      colnames(XO) <- rep("XO", NXO)
   nObs <- length( YS)
   i0 <- YS==0
   i1 <- YS==1
   NO1 <- length( YS[i0])
   NO2 <- length( YS[i1])
   if(!is.null(weights)) {
      warning("Argument 'weight' is ignored by tobitTfit")
   }
   ## indices in for the parameter vector
   if(is.null(index)) {
      iBetaS <- 1:NXS
      iBetaO <- max(iBetaS) + seq(length=NXO)
      if(!binaryOutcome) {
         iSigma <- max(iBetaO) + 1
         iRho <- max(iSigma) + 1
      }
      else
         iRho <- max(iBetaO) + 1
      nParam <- iRho
   }
   else {
      iBetaS <- index$betaS
      iBetaO <- index$betaO
      iSigma <- index$errTerms["sigma"]
      iRho <- index$errTerms["rho"]
      nParam <- index$nParam
   }
   ## split the data by selection
   XS0 <- XS[i0,,drop=FALSE]
   XS1 <- XS[i1,,drop=FALSE]
   YO0 <- YO[i0]
   YO1 <- YO[i1]
   XO0 <- XO[i0,,drop=FALSE]
   XO1 <- XO[i1,,drop=FALSE]
   ##
   if(print.level > 0) {
      cat( "Non-participants: ", NO1,
          "; participants: ", NO2, "\n", sep="")
      cat( "Initial values:\n")
      cat("selection equation betaS:\n")
      print(start[iBetaS])
      cat("Outcome equation betaO\n")
      print(start[iBetaO])
      cat("Variance sigma\n")
      print(start[iSigma])
      cat("Correlation rho\n")
      print(start[iRho])
   }
   result <- maxLik(loglik,
                    grad=gradlik,
                    hess=hesslik,
                    start=start,
                    print.level=print.level,
                    method=maxMethod,
                    ...)
   ## compareDerivatives(#loglik,
   ##     gradlik,
   ##     hesslik,
   ##                    t0=start)
   result$tobitType <- "treatment"
   result$method <- "ml"
   class( result ) <- c( "selection", class( result ) )
   return( result )
}

treatReg <- function(selection, outcome,
                      data=sys.frame(sys.parent()),
                      weights = NULL,
                      subset,
                      method="ml",
                      start=NULL,
                      ys=FALSE, xs=FALSE,
                      yo=FALSE, xo=FALSE,
                      mfs=FALSE, mfo=FALSE,
                      print.level=0,
                      ...) {
   ## Heckman-style treatment effect models
   ## selection:   formula
   ##              LHS: must be convertable to two-level factor (e.g. 0-1, 1-2, "A"-"B")
   ##              RHS: ordinary formula as in lm()
   ## outcome:     formula
   ##              should include selection outcome
   ## ys, xs, yo, xo, mfs, mfo: whether to return the response, model matrix or
   ##              the model frame of outcome and selection equation(s)
   ## First the consistency checks
   ## ...          additional arguments for tobit2fit and tobit5fit
   type <- 0
   if(!inherits( selection, "formula" )) {
      stop( "argument 'selection' must be a formula" )
   }
   if( length( selection ) != 3 ) {
      stop( "argument 'selection' must be a 2-sided formula" )
   }
   if(inherits(outcome, "formula")) {
      if( length( outcome ) != 3 ) {
         stop( "argument 'outcome' must be a 2-sided formula" )
      }
   }
   else
       stop("argument 'outcome' must be a formula" )
   if(!missing(data)) {
      if(!inherits(data, "environment") & !inherits(data, "data.frame") & !inherits(data, "list")) {
         stop("'data' must be either environment, data.frame, or list (currently a ", class(data), ")")
      }
   }
   ##
   if(print.level > 0)
       cat("Treatment effect model", type, "model\n")
   probitEndogenous <- model.frame( selection, data = data)[ , 1 ]
   probitLevels <- levels( as.factor( probitEndogenous ) )
   if( length( probitLevels ) != 2 ) {
      stop( "the left hand side of 'selection' has to contain",
         " exactly two levels (e.g. FALSE and TRUE)" )
   }
   if( !is.null( weights )) {
      warning( "argument 'weights' is ignored" )
      weights <- NULL
   }
   ## now check whether two-step method was requested
   cl <- match.call()
   if(method == "2step") {
      twoStep <- heckitTfit(selection, outcome, data=data,
#                            weights = weights,
                            print.level = print.level, ... )
      twoStep$call <- cl
      class(twoStep) <- c("selection", class(twoStep))
      return(twoStep)
   }
   ## Now extract model frames etc
   ## YS (selection equation)
   mf <- match.call(expand.dots = FALSE)
   m <- match(c("selection", "data", "subset"), names(mf), 0)
   mfS <- mf[c(1, m)]
   mfS$drop.unused.levels <- TRUE
   mfS$na.action <- na.pass
   mfS[[1]] <- as.name("model.frame")
   names(mfS)[2] <- "formula"
                                        # model.frame requires the parameter to
                                        # be 'formula'
   mfS <- eval(mfS, parent.frame())
   mtS <- attr(mfS, "terms")
   XS <- model.matrix(mtS, mfS)
   YS <- model.response(mfS)
   YSLevels <- levels( as.factor( YS ) )
   if( length( YSLevels ) != 2 ) {
      stop( "the left hand side of the 'selection' formula has to contain",
         " exactly two levels (e.g. FALSE and TRUE)" )
   }
   YS <- as.integer(YS == YSLevels[ 2 ])
                                        # selection will be kept as integer internally
   ## check for NA-s.  Because we have to find NA-s in several frames, we cannot use the standard na.
   ## functions here.  Find bad rows and remove them later.
   ## We check XS and YS separately, because mfS may be a data frame with complex structure (e.g.
   ## including matrices)
   badRow <- !complete.cases(YS, XS)
   badRow <- badRow | is.infinite(YS)
   badRow <- badRow | apply(XS, 1, function(v) any(is.infinite(v)))
   ## YO (outcome equation)
   ## Here we should include a possibility for the user to
   ## specify the model.  Currently just a guess.
   binaryOutcome <- FALSE
   oArg <- match("outcome", names(mf), 0)
                           # find the outcome argument
   m <- match(c("outcome", "data", "subset",
                "offset"), names(mf), 0)
   ## replace the outcome list by the first equation and evaluate it
   mfO <- mf[c(1, m)]
   mfO$drop.unused.levels <- TRUE
   mfO$na.action <- na.pass
   mfO[[1]] <- as.name("model.frame")
                           # eval it as model frame
   names(mfO)[2] <- "formula"
   mfO <- eval(mfO, parent.frame())
                           # Note: if unobserved variables are
                           # marked as NA, eval returns a
                           # subframe of visible variables only.
                           # We have to check it later
   mtO <- attr(mfO, "terms")
   XO <- model.matrix(mtO, mfO)
   YO <- model.response(mfO)
   if(is.logical(YO) |
      (is.factor(YO) & length(levels(YO)) == 2)) {
      binaryOutcome <- TRUE
   }
   ## Now figure out if selection outcome is in fact used as
   ## explanatory variable for the outcome
   selectionVariable <- as.character(selection[[2]])
                           # name of the selection outcome
   ##
   badRow <- badRow | !complete.cases(YO, XO)
   badRow <- badRow | is.infinite(YO)
   badRow <- badRow | apply(XO, 1, function(v) any(is.infinite(v)))
                           # outcome cases that contain NA, Inf, NaN
   if( !is.null( weights ) ) {
      if( length( weights ) != length( badRow ) ) {
         stop( "number of weights (", length( weights ), ") is not equal",
              " to the number of observations (", length( badRow ), ")" )
      }
      badRow <- badRow | is.na( weights )
      badRow <- badRow | is.infinite( weights )
   }   
   if(print.level > 0) {
      cat(sum(badRow), "invalid observations\n")
   }
   if( method == "model.frame" ) {
      mf <- mfS
      mf <- cbind( mf, mfO[ , ! names( mfO ) %in% names( mf ), drop = FALSE ] )
      return( mf[ !badRow, ] )
   }
   XS <- XS[!badRow,, drop=FALSE]
   YS <- YS[!badRow]
   XO <- XO[!badRow,, drop=FALSE]
   YO <- YO[!badRow]
   weightsNoNA <- weights[ !badRow ]
   NXS <- ncol(XS)
   NXO <- ncol(XO)
   ## parameter indices in the parameter vector
   iBetaS <- seq(length=ncol(XS))
   iBetaO <- max(iBetaS) + seq(length=NXO)
   if(!binaryOutcome) {
      iSigma <- max(iBetaO) + 1
      iRho <- max(iSigma) + 1
   }
   else
      iRho <- max(iBetaO) + 1
   nParam <- iRho
   if(binaryOutcome) {
      iErrTerms <- c(rho=iRho)
   }
   else {
      iErrTerms <- c(sigma=iSigma, rho=iRho )
   }
   index <- list(betaS=iBetaS,
                 betaO=iBetaO,
                 errTerms=iErrTerms,
                 outcome = iBetaO,
                 nParam=iRho)
   ##
   twoStep <- NULL
   if(is.null(start)) {
                           # start values by Heckman 2-step method
      start <- numeric(nParam)
      twoStep <- heckitTfit(selection, outcome, data=data,
                            print.level = print.level,
#                            weights = weights
                            )
      coefs <- coef(twoStep, part="full")
      start[iBetaS] <- coefs[twoStep$param$index$betaS]
      if(!binaryOutcome) {
         start[iBetaO] <- coefs[twoStep$param$index$betaO]
         start[iSigma] <- coefs[twoStep$param$index$sigma]
      }
      else
         start[iBetaO] <- coefs[twoStep$param$index$betaO]/coefs[twoStep$param$index$sigma]
      start[iRho] <- coefs[twoStep$param$index$rho]
      if(start[iRho] > 0.99)
         start[iRho] <- 0.99
      else if(start[iRho] < -0.99)
         start[iRho] <- -0.99
   }
   if(is.null(names(start))) {
      if(!binaryOutcome) {
         names(start) <- c(colnames(XS), colnames(XO), "sigma",
                           "rho")
      }
      else
         names(start) <- c(colnames(XS), colnames(XO), 
                           "rho")
   }                                        # add names to start values if not present
   if(!binaryOutcome) {
      estimation <- tobitTfit(YS, XS, YO, XO, start,
#                              weights = weightsNoNA,
                              print.level=print.level,
                              index=index,
                              binaryOutcome=binaryOutcome,
                              ...)
   }
   else {
      ## estimation <- tobitTBfit(YS, XS, YO, XO, start, weights = weightsNoNA,
      ##                          print.level=print.level, ...)
      ## iErrTerms <- c(rho=iRho)
      stop("Binary outcome models are not implemented")
   }
   param <- list(index=index,
                 NXS=ncol(XS), NXO=ncol(XO),
                 N0=sum(YS==0), N1=sum(YS==1),
                 nObs=length(YS), nParam=length(start),
                 df=length(YS) - length(start),
                 levels=YSLevels,
                           # levels[1]: selection 1; levels[2]:
                           # selection 2
                 selectionVariableName=selectionVariable
                           # which explanatory variable is selection outcome
                 )
   result <- c(estimation,
               twoStep=list(twoStep),
               start=list(start),
               param=list(param),
               call=cl,
               termsS=mtS,
               termsO=mtO,
               ys=switch(as.character(ys), "TRUE"=list(YS), "FALSE"=NULL),
               xs=switch(as.character(xs), "TRUE"=list(XS), "FALSE"=NULL),
               yo=switch(as.character(yo), "TRUE"=list(YO), "FALSE"=NULL),
               xo=switch(as.character(xo), "TRUE"=list(XO), "FALSE"=NULL),
               mfs=switch(as.character(mfs), "TRUE"=list(mfS[!badRow,]), "FALSE"=NULL),
               mfo=switch(as.character(mfs),
               "TRUE"=list(mfO[!badRow,]), "FALSE"=NULL)
               )
   result$binaryOutcome <- binaryOutcome
   class( result ) <- class( estimation ) 
   return(result)
}
```

##Treatment effect model equation
```{r}
treatmodel<- treatReg(ServiceModuleCompleted~PT + factor(SA_gender) + log(SA.Pay) +SA.service_yrs,log(Annual.SaleAmt)~num_PKModulesCompleted*ServiceModuleCompleted +factor(SA_gender) + SA.service_yrs + Cust.GenderRatio+PT+factor(MallGrade), data=yr2013, method="ML") 


treatmodel2<- treatReg(ServiceModuleCompleted~PT + factor(SA_gender) + log(SA.Pay) +SA.service_yrs,log(Annual.SaleAmt)~ factor(SA_gender) + SA.service_yrs + Cust.GenderRatio+PT+factor(MallGrade), data=yr2013, method="ML") 

summary(treatmodel)

plot(effect(term="num_PKModulesCompleted:ServiceModuleCompleted",mod=treatmodel,default.levels=2),multiline=TRUE)

library(lmtest)
lrtest(treatmodel2,treatmodel)
```
Pick the first treatment effect model to predict whether a salesperson completes the service module training.
